{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pT5Bp4lvOAX9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iiithat/AIMLtraining_AT/blob/main/Day_06-Speech_AIML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech Processing\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/0*Vji8JnX0o0wB0qVR.png\" alt=\"speech-rep\" width=\"400\"/>  \n",
        "\n"
      ],
      "metadata": {
        "id": "pT5Bp4lvOAX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io"
      ],
      "metadata": {
        "id": "WPvIo06AQZ5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhVPtDqTQct1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is a Speech signal? What makes it different from a normal signal?***\n",
        "\n",
        "A speech signal is a type of audio signal, but it has some specific characteristics that make it different from other types of audio signals.\n",
        "\n",
        "- **Non-stationarity**: Speech signals are non-stationary, meaning that their statistical properties change over time. This is because speech signals are made up of a sequence of different sounds, such as vowels, consonants, and pauses. Each of these sounds has different statistical properties. Other types of audio signals, such as music or sound effects, may be more stationary.\n",
        "\n",
        "- **Frequency range**: Speech signals typically have a frequency range of 80 Hz to 10 kHz, although the most important part of the signal is in the range of 200 Hz to 3500 Hz. This is the frequency range of human hearing, so it is the range of frequencies that are most important for understanding speech. Other types of audio signals, such as music or sound effects, may have a wider frequency range.\n",
        "\n",
        "- **Dynamic range**: Speech signals have a very wide dynamic range, meaning that the difference between the loudest and softest parts of the signal can be very large. This is because speech signals can vary in loudness depending on the speaker, the distance of the speaker from the microphone, and the environment in which the speech is being recorded. Other types of audio signals may have a narrower dynamic range.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "db-kIatRKKgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do humans produce speech?**\n",
        "\n",
        "Speech is a complex and dynamic motor activity that allows us to express our thoughts and emotions. It conveys linguistic and non-linguistic information, such as the speaker's identity, gender, emotional state, health condition, language, and environmental conditions. The speech production mechanism is a complex process involving many muscles coordinated activities. It is controlled by the brain, which sends signals to the muscles to produce the desired sounds. Various organs like lungs, trachea, larynx, pharyngeal cavity,\n",
        "oral cavity, nasal cavity and lips are involved in speech production.It can be broadly divided into sub-processes such as Respiration, Phonation, and Articulation.\n",
        "\n",
        "**Respiration**: The first step in speech production is respiration. The lungs are the primary source of air for speech and are responsible for providing the air pressure needed to vibrate the vocal folds. The air pressure generated depends on the respiratory muscles strength and the lungs size. Lungs, ribs, chest muscles, and diaphragm are responsible for respiration sub-process. The loudness of the sound is determined by the amount of air pressure that is generated. Air produced in the lungs generates energy for respiration and  reaches the larynx. The phonation process is initiated at the larynx.\n",
        "\n",
        "**Phonation**: The second step in speech production is phonation. This is the process of producing sound by vibrating the vocal folds. The vocal folds are two folds of tissue in the larynx. When the vocal folds are held together, and the air is passed through them, they vibrate. The frequency of the vibrations determines the pitch of the sound. The pitch of the sound can be varied by changing the tension of the vocal folds or by changing the amount of air pressure generated. Depending on the functioning of vocal folds, different phonations can be experienced by which different sounds are produced. Some of the phonation types are Modal phonation, Creaky phonation, Breathy phonation, Falsetto phonation, Harsh phonation, Whisper phonation.\n",
        "\n",
        "**Articulation**: The third step in speech production is articulation. This is the process of modifying the sound produced by the vocal folds by changing the shape of the vocal tract. The vocal tract is the space from the larynx to the lips. The tongue, lips, teeth, and jaw can change the shape of the vocal tract. Different vocal tract shapes create different vowels and consonants in speech.\n",
        "For example, the vowel sound /a/ is produced by a relatively open vocal tract, while the vowel sound /i/ is produced by a relatively closed vocal tract. The consonant sound /p/ is produced by closing the lips, while the consonant sound /m/ is produced by closing the lips and lowering the velum (the soft palate). Each process interacts and influences one another during speech production.\n",
        "<!-- <img src=\"https://miro.medium.com/max/1400/1*BzhKcJJxv974OxWOVqUuQQ.png\"> -->\n",
        "<!-- <img src= \"https://static.wixstatic.com/media/00d3b0_5fdb171e9d69422b9b6a589f30f2feba.png/v1/fill/w_350,h_255,al_c,lg_1,q_85,enc_auto/00d3b0_5fdb171e9d69422b9b6a589f30f2feba.png\">  \n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*tzimNnoSuF8UH279hi6pQA.jpeg\" width=\"600\"/> -->\n",
        "<img src=\"https://i.ibb.co/f1BzmTX/IMG-20230922-WA0008.jpg\" width=\"400\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R7ka8nfiKdzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does a machine see a signal?**\n",
        "\n",
        "We are familiar with how a computer/machine looks at an image: as a 2D/3D matrix of numbers which correspond to the intensity of the color. So similarly how does a computer store a signal?\n",
        "\n",
        "We know that a signal is continuous in nature. But we cannot store continuous data, but only discrete values. So how do we convert an analog continuous signal into a digital discrete signal?\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M7ziiYu9oJt3zCQEeVelxQ.jpeg\" width=\"600\"/>"
      ],
      "metadata": {
        "id": "2fzIclw1KVDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What all information does the speech signal hold?**\n",
        "\n",
        "When a person speaks, what all can we infer by listening to them? Do an experiment: blindfold yourself and listen carefully to the voice of person (preferably a stranger). What all information can you gather from it? You will notice that there is quite a lot more than just the content in speech.\n",
        "\n",
        "**Linguistic Information**\n",
        "  - Content: The actual information that the person is trying to convey as a series of sentences.\n",
        "  - Language: By analyzing the sentences, we can tell the language in which these sentences are spoken in.\n",
        "  - Ethnicity/Accent: In some cases, a single language can be spoken in different accents from which we can estimate the ethinicity.\n",
        "  - Emotional state: We can also tell the emotion in which the person is conveying the sentences, whether he is angry, sad, or neutral, etc.\n",
        "  - Social context: By looking at the content and the structure of the sentences, we can up to an extent, also tell for example if they are talking to a friend, colleague, or customer.\n",
        "\n",
        "**Acoustic Information**: The speech signal also contains information about the environment in which it was recorded, such as the background noise and the distance between the speaker and the microphone.  \n",
        "\n",
        "**Speaker Information**:\n",
        "  - Identity: If we know the person speaking previously, then we will be able to recognize them from just listening to their speech.\n",
        "  - Gender: If we dont know the person previously, then we can atleast identify their gender.\n",
        "  - Age: We also can, up to an extent, guess the age of the person speaking.\n",
        "  - Disorders and Health: Variations in speech patterns can indicate various health conditions and disorders, such as stuttering, dysarthria, or voice disorders."
      ],
      "metadata": {
        "id": "nz9xwgmzKnWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intro to Speech Processing**\n",
        "\n",
        "In this section, we will briefly introduce basic terms that are used in speech signal processing\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:640/0*cIqWsFl9x8JLLzAJ.gif\" width=\"600\"/>  "
      ],
      "metadata": {
        "id": "t4_HBO1WK0Ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction**\n",
        "\n",
        "**Pre-emphasis**\n",
        "\n",
        "Pre-emphasis boosts the amount of energy in the high frequencies. For voiced segments like vowels, there is more energy at the lower frequencies than the higher frequencies. This is called spectral tilt which is related to the glottal source (how vocal folds produce sound). Boosting the high-frequency energy makes information in higher formants more available to the acoustic model. This improves phone detection accuracy. For humans, we start having hearing problems when we cannot hear these high-frequency sounds. Also, noise has a high frequency. In the engineering field, we use pre-emphasis to make the system less susceptible to noise introduced in the process later. For some applications, we just need to undo the boosting at the end.  \n",
        "Pre-emphasis uses a filter to boost higher frequencies. Below is the before and after signal on how the high-frequency signal is boosted.\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*BgDnOWAm5X_xmKCncdgS7g.jpeg\" width=\"600\"/>\n",
        "\n",
        "***Fourier Transform, DFT and FFT***\n",
        "\n",
        "The Fourier Transform is a mathematical formula that allows us to decompose a signal into it's individual frequencies and the frequency's amplitude. In other words, it converts the signal from the time domain into the frequency domain. The result is called a spectrum.  \n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTYCtcx_7otHVu-uToI9dA.png\" width=\"600\"/>  \n",
        "The Fourier Transform is designed for a continous signal. As we already discussed computers can only store discrete information, so we use a modified version of FT called the Discrete Fourier Transform (DFT). Here is the DFT formula:\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gEg1OmRy1YbnQB9kQuOL_Q.jpeg\" width=\"600\"/>  \n",
        "The Fast Fourier Transform (FFT) is an algorithm that can efficiently compute the Discrete Fourier Transform, and is the foundation of all signal processing.\n",
        "\n",
        "**Windowing**\n",
        "Windowing involves the slicing of the audio waveform into sliding frames. We have hyper parameters such as window length, hop length, overlap length, window type (such as Hann, Hamming) for performing windowing operation.\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pYPjNG7naNRP1Og0IivqXw.jpeg\" width=\"600\"/>  \n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Zc4TTs7m-vlEEJWq.png\" width=\"600\"/>  \n",
        "\n",
        "**STFT and Linear Spectrogram**\n",
        "FFT is only defined for periodic signals, where as speech signal is a non periodic signal. But luckily, speech is quasi-stationary, meaning it is periodic in a small duration. Therefore we can still compute FFT for a speech signal, but we apply for small durations. We already have an operation that does this: Windowing. So if we first do windowing and then for each window if we apply FFT, we get STFT (Short Term Fourier Transform) and the output is called a Spectrogram (more specifically Linear Spectrogram).  \n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*r3rkXXuwvAjwZli5.png\" width=\"600\"/>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*tIBRdtG3EfjmSIlraWVIxw.png\" width=\"600\"/>\n",
        "\n",
        "**Mel Scale and Spectrogram**\n",
        "Studies have shown that humans do not perceive frequencies on a linear scale. For humans, the perceived loudness changes according to frequency. We are better at detecting differences in lower frequencies than higher frequencies. For example, we can easily tell the difference between 500 and 1000 Hz, but we will hardly be able to tell a difference between 10,000 and 10,500 Hz, even though the distance between the two pairs are the same. So researchers have devised a new scale called the Mel scale which captures this.  \n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:640/1*erUKb2-Z-Wi_u8JWel6cDQ.gif\" width=\"200\"/>  \n",
        "A mel spectrogram is a spectrogram where the frequencies are converted to the mel scale. There are algorithms that convert a linear frequency range to a mel frequency range. All these mappings are non-linear. In feature extraction, we apply triangular band-pass filters to coverts the frequency information to mimic what a human perceived.  \n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kiho3g2yqLuojsADZsQQNA.png\" width=\"600\"/>  \n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dTAxtUkRDwbmRJ3c.png\" width=\"600\"/>  \n",
        "\n",
        "***MFCC***\n",
        "Mel-frequency cepstral coefficients (MFCCs) are used to represent the spectral characteristics of sound in a way that is well-suited for various machine learning tasks, such as speech recognition and music analysis.\n",
        "\n",
        "The MFCC extraction process consists of the following steps: Pre-emphasis, Framing, Windowing, FFT, Mel-frequency filtering, Logarithm, DCT\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hfXwbKUCfDITmDQtI1H5A.jpeg\" width=\"600\"/>  "
      ],
      "metadata": {
        "id": "H8MPo3uOLIq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Application of Speech Processing***\n",
        "Speech processing is a field of study and technology that deals with the analysis, synthesis, and manipulation of speech signals. It has a wide range of applications across various domains. Some of them are:\n",
        "\n",
        "***Automatic Speech Recognition (ASR):*** ASR systems convert spoken language into text. They are used in voice assistants (e.g., Siri, Alexa), transcription services, and voice command recognition.\n",
        "\n",
        "***Text-to-Speech (TTS) Synthesis:*** TTS systems generate spoken language from text. They are used in navigation systems, audiobooks, and accessibility tools for the visually impaired.\n",
        "\n",
        "***Speaker Recognition:*** This technology verifies or identifies individuals based on their unique vocal characteristics. It is used in security systems, access control, and forensic applications.\n",
        "\n",
        "***Emotion Recognition:*** Speech processing can be used to detect emotions from speech, which has applications in customer service (sentiment analysis), mental health monitoring, and human-computer interaction.\n",
        "\n",
        "***Language Identification:*** Language identification systems determine the language spoken in an audio clip. They are used in multilingual call centers, content filtering, and translation services.\n",
        "\n",
        "***Speaker Diarization:*** This process segments an audio recording into distinct speaker segments, which is useful in transcription services, meeting analysis, and voice-controlled transcription software.\n",
        "\n",
        "***Language Translation:*** Speech-to-speech translation systems translate spoken language from one language to another in real time, facilitating cross-lingual communication.\n",
        "\n",
        "***Voice Assistants:*** Voice-controlled personal assistants (e.g., Siri, Google Assistant) use speech processing for understanding user commands and providing responses and actions.\n",
        "\n",
        "These are just some examples of the many applications of speech processing technology."
      ],
      "metadata": {
        "id": "dRMcaDiuK6FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "Kd9bSqIP03wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For demo purpose, Speaker Recognition task is being done.\n",
        "Speaker recognition, also known as voice recognition or speaker identification, is a technology that involves identifying or verifying an individual based on their unique vocal characteristics. This technology is often used for security, authentication, and various other applications. There are two primary types of speaker recognition:\n",
        "\n",
        "    Speaker Identification:\n",
        "        Identification: In speaker identification, the system determines the identity of a speaker by comparing their voice to a database of known speakers. The system uses various acoustic features of the voice, such as pitch, speech rate, spectral features, and more, to create a unique voiceprint for each individual.\n",
        "        Applications: Speaker identification is used in applications like access control, voice-based user authentication (e.g., unlocking a smartphone), and in secure facilities where verifying the identity of a speaker is crucial.\n",
        "\n",
        "    Speaker Verification:\n",
        "        Verification: In speaker verification, the system verifies if a person is who they claim to be by comparing their voice to a previously enrolled voiceprint. This is often used in two-factor authentication, where something the user knows (e.g., a PIN) is combined with something the user is (their voice).\n",
        "        Applications: Speaker verification is used in various security applications, including phone-based banking transactions, customer service authentication, and secure voice commands on devices like smart speakers."
      ],
      "metadata": {
        "id": "CwE6GM_k08Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker Recognition using GMM:\n"
      ],
      "metadata": {
        "id": "TkWfFcOb0prK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install joblib librosa==0.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AUS5Tcb1JUP",
        "outputId": "5b0e07f7-76db-4eba-9c35-e68abcdbd9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import scipy.io.wavfile as wave\n",
        "from scipy.fftpack import fft, ifft, fftshift\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import mixture\n",
        "import joblib"
      ],
      "metadata": {
        "id": "gEE644P01AVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "chCAdGHv0H69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bca4ffd-2be6-4d5a-b216-178c6d7762f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR_ZIP = \"/content/gdrive/MyDrive/Lab/SR_data.zip\"\n",
        "! unzip -q {DATA_DIR_ZIP} -d .\n",
        "DATA_DIR = \"/content/Speaker_Recognition_Data\""
      ],
      "metadata": {
        "id": "FsD5R2NQ08Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def traininig(\n",
        "    train_data_path,\n",
        "    feat_train_path,\n",
        "    trained_model_path,\n",
        "    window_overlap = 0.005,\n",
        "    n_mixtures = 16,\n",
        "    max_iterations = 200,\n",
        "    calc_deltas=True,\n",
        "    sr=8000,\n",
        "):\n",
        "    hop_length=int(0.005*sr)\n",
        "    all_speakers=glob.glob(train_data_path + '/*')\n",
        "\n",
        "    if not os.path.exists(feat_train_path):\n",
        "        os.makedirs(feat_train_path)\n",
        "    if not os.path.exists(trained_model_path):\n",
        "        os.makedirs(trained_model_path)\n",
        "\n",
        "\n",
        "    for itr1 in range(0,len(all_speakers)):\n",
        "        wavs=glob.glob(all_speakers[itr1]+'/*.wav')\n",
        "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
        "        final_feat=np.empty([0, 39])\n",
        "\n",
        "        print(spk)\n",
        "        for itr2 in range(0,len(wavs)):\n",
        "            y, sr_ = librosa.load(wavs[itr2])\n",
        "            y = librosa.resample(y, sr_, sr)\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
        "            mfcc_delta = librosa.feature.delta(mfcc)\n",
        "            mfcc_ddelta = librosa.feature.delta(mfcc_delta)\n",
        "            feat=np.concatenate((mfcc,mfcc_delta,mfcc_ddelta),axis=0)\n",
        "            feat=feat.transpose()\n",
        "            final_feat=np.concatenate((final_feat,feat),axis=0)\n",
        "\n",
        "        np.savetxt(os.path.join(feat_train_path, spk + \"_all_features.txt\"), final_feat, delimiter=\",\")\n",
        "        try:\n",
        "            gmm = mixture.GaussianMixture(n_components=n_mixtures, covariance_type='diag' , max_iter=max_iterations).fit(final_feat)\n",
        "        except:\n",
        "            print(\"ERROR : Error while training model for file \"+spk)\n",
        "        try:\n",
        "            joblib.dump(gmm, os.path.join(trained_model_path, spk + '.pkl'))\n",
        "        except:\n",
        "            print(\"ERROR : Error while saving model for \"+spk)\n",
        "    print(\"Training Completed\")"
      ],
      "metadata": {
        "id": "ClXupfWq1I1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(\n",
        "    test_data_path,\n",
        "    feat_test,\n",
        "    trained_model_path,\n",
        "    sr=8000,\n",
        "):\n",
        "    hop_length=int(0.005*sr)\n",
        "    all_speakers=glob.glob(test_data_path + '/*')\n",
        "\n",
        "    if not os.path.exists(feat_test):\n",
        "        os.makedirs(feat_test)\n",
        "\n",
        "    speakers = {speaker: ind for ind, speaker in enumerate(all_speakers)}\n",
        "\n",
        "    num_test_cases={}\n",
        "    tct={}\n",
        "    for e in speakers:\n",
        "        num_test_cases[os.path.basename(e)] = len(os.listdir(e)) - 1\n",
        "        tct[os.path.basename(e)] = 0\n",
        "\n",
        "    spk_names = {os.path.basename(key): val for key, val in speakers.items()}\n",
        "    total_speakers = len(spk_names)\n",
        "    confusion_matrix = np.zeros((total_speakers, total_speakers))\n",
        "\n",
        "    for itr1 in range(0,len(all_speakers)):\n",
        "        wavs=glob.glob(all_speakers[itr1] + '/*.wav')\n",
        "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
        "        final_feat=np.empty([0, 39])\n",
        "\n",
        "        print(spk)\n",
        "        for itr2 in range(0,len(wavs)):\n",
        "            y, sr_ = librosa.load(wavs[itr2])\n",
        "            y = librosa.resample(y, sr_, sr)\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
        "            mfcc_delta = librosa.feature.delta(mfcc)\n",
        "            mfcc_ddelta = librosa.feature.delta(mfcc_delta)\n",
        "            feat=np.concatenate((mfcc,mfcc_delta,mfcc_ddelta),axis=0)\n",
        "            feat=feat.transpose()\n",
        "            final_feat=np.concatenate((final_feat,feat),axis=0)\n",
        "\n",
        "            max_score=-np.inf\n",
        "            max_spk_name=\"\"\n",
        "\n",
        "            for modelfile in sorted(glob.glob(trained_model_path + '/*.pkl')):\n",
        "                gmm = joblib.load(modelfile)\n",
        "                score=gmm.score(feat)\n",
        "                if score>max_score:\n",
        "                    max_score = score\n",
        "                    max_spk_name = os.path.splitext(os.path.basename(modelfile))[0]\n",
        "\n",
        "            print(spk+\" -> \"+max_spk_name+(\" Y\" if spk==max_spk_name  else \" N\"))\n",
        "            confusion_matrix[ spk_names[spk] ][spk_names[max_spk_name]]+=1\n",
        "            tct[spk]+=1\n",
        "\n",
        "        np.savetxt(feat_test+spk+\"_all_features.txt\", feat, delimiter=\",\")\n",
        "    return tct,confusion_matrix,total_speakers"
      ],
      "metadata": {
        "id": "tkfy_va31Js0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r {DATA_DIR}"
      ],
      "metadata": {
        "id": "WosDN1Bt66p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=os.path.join(DATA_DIR, \"traindata\")\n",
        "test_data=os.path.join(DATA_DIR, \"testdata\")\n",
        "feat_train=os.path.join(DATA_DIR, \"feat/train\")\n",
        "feat_test=os.path.join(DATA_DIR, \"feat/test\")\n",
        "trained_model=os.path.join(DATA_DIR, \"train_models\")"
      ],
      "metadata": {
        "id": "BoiCyAzC1Lu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traininig(train_data, feat_train, trained_model)"
      ],
      "metadata": {
        "id": "Z7jd4qSK1N4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c876392-eedf-4afa-b6bd-775ce44116ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fdfb0\n",
            "fjlr0\n",
            "fjlg0\n",
            "feme0\n",
            "fdjh0\n",
            "fgcs0\n",
            "fcmg0\n",
            "fcke0\n",
            "falk0\n",
            "fgrw0\n",
            "Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tt, conf_mat, tot_spek = testing(test_data, feat_test, trained_model)"
      ],
      "metadata": {
        "id": "NDJvV43W1SKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18728042-c3ee-4d26-a0d4-6336e9b807bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fdfb0\n",
            "fdfb0 -> fcmg0 N\n",
            "fdfb0 -> fdfb0 Y\n",
            "fjlr0\n",
            "fjlr0 -> fjlr0 Y\n",
            "fjlr0 -> fjlr0 Y\n",
            "fjlg0\n",
            "fjlg0 -> fjlg0 Y\n",
            "fjlg0 -> fjlg0 Y\n",
            "feme0\n",
            "feme0 -> feme0 Y\n",
            "feme0 -> feme0 Y\n",
            "fdjh0\n",
            "fdjh0 -> falk0 N\n",
            "fdjh0 -> falk0 N\n",
            "fgcs0\n",
            "fgcs0 -> feme0 N\n",
            "fgcs0 -> fgcs0 Y\n",
            "fcmg0\n",
            "fcmg0 -> fcmg0 Y\n",
            "fcmg0 -> fcmg0 Y\n",
            "fcke0\n",
            "fcke0 -> fcke0 Y\n",
            "fcke0 -> fcke0 Y\n",
            "falk0\n",
            "falk0 -> falk0 Y\n",
            "falk0 -> falk0 Y\n",
            "fgrw0\n",
            "fgrw0 -> fgrw0 Y\n",
            "fgrw0 -> fgrw0 Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tt)\n",
        "print(\"Confusion Matrix:\\n\",conf_mat)\n",
        "print(\"Accuracy: \",(sum([ conf_mat[i][j] if i==j  else 0 for i in range(tot_spek) for j in range(tot_spek) ] )*100)/float(sum([i for i in tt.values()])))"
      ],
      "metadata": {
        "id": "Ci0k_geG1U2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "becb5cf4-45c1-4dde-a334-f269d87df106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdfb0': 2, 'fjlr0': 2, 'fjlg0': 2, 'feme0': 2, 'fdjh0': 2, 'fgcs0': 2, 'fcmg0': 2, 'fcke0': 2, 'falk0': 2, 'fgrw0': 2}\n",
            "Confusion Matrix:\n",
            " [[1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 2. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 2. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 2. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]]\n",
            "Accuracy:  80.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker Recognition using DNN"
      ],
      "metadata": {
        "id": "a2EssJUh1Wjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD, Adam, RMSprop,Adadelta\n",
        "import numpy as np\n",
        "import joblib\n",
        "import glob\n",
        "import os\n",
        "import keras\n",
        "import librosa"
      ],
      "metadata": {
        "id": "iGT_giuL0H9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feat_ext(\n",
        "    feat_path,\n",
        "    data_path,\n",
        "    window_overlap = 0.005,\n",
        "    n_mixtures = 32,\n",
        "    max_iterations = 200,\n",
        "    calc_deltas=True,\n",
        "    sr=8000\n",
        "):\n",
        "    all_speakers=glob.glob(data_path + '/*')\n",
        "    hop_length=int(0.005*sr)\n",
        "\n",
        "    if not os.path.exists(feat_path):\n",
        "        os.makedirs(feat_path)\n",
        "\n",
        "    for itr1 in range(0,len(all_speakers)):\n",
        "        wavs=glob.glob(all_speakers[itr1]+'/*.wav')\n",
        "        spk=(all_speakers[itr1]).split(\"/\")[-1]\n",
        "        final_feat=np.empty([0, 39])\n",
        "\n",
        "        print(spk)\n",
        "        for itr2 in range(0,len(wavs)):\n",
        "            y, sr_ = librosa.load(wavs[itr2])\n",
        "            y = librosa.resample(y, sr_, sr)\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
        "            mfcc_delta = librosa.feature.delta(mfcc)\n",
        "            mfcc_ddelta = librosa.feature.delta(mfcc_delta)\n",
        "            feat=np.concatenate((mfcc,mfcc_delta,mfcc_ddelta),axis=0)\n",
        "            feat=feat.transpose()\n",
        "            final_feat=np.concatenate((final_feat,feat),axis=0)\n",
        "        np.savetxt(os.path.join(feat_path, spk+\"_all_features.txt\"), final_feat, delimiter=\",\")\n",
        "    speakers = {os.path.basename(all_speakers[k]): k for k in range(len(all_speakers)) }\n",
        "    return speakers"
      ],
      "metadata": {
        "id": "vJ_A3hXc1bED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dnn_model(\n",
        "    n_input,\n",
        "    n_output,\n",
        "    n_hidden,\n",
        "    speakers,\n",
        "    train_feat_path,\n",
        "    n_epochs=10,\n",
        "    b_size=20,\n",
        "    n_cross_val=1,\n",
        "):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n_hidden, input_shape=(n_input,),activation='relu'))\n",
        "    model.add(Dense(n_output,activation='softmax'))\n",
        "\n",
        "    x_train,y_train=[],[]\n",
        "    for spk in speakers:\n",
        "        all_spk_data=np.genfromtxt(os.path.join(train_feat_path, spk+\"_all_features.txt\"), delimiter=',')\n",
        "        print(all_spk_data.shape)\n",
        "        all_spk_labels = []\n",
        "        for i in range(all_spk_data.shape[0]):\n",
        "            d = np.zeros(len(speakers))\n",
        "            d[speakers[spk]]=1\n",
        "            all_spk_labels.append(d)\n",
        "        all_spk_labels=np.array(all_spk_labels)\n",
        "\n",
        "        for i in range(all_spk_data.shape[0]):\n",
        "            x_train.append(all_spk_data[i])\n",
        "            y_train.append(all_spk_labels[i])\n",
        "    x_train=np.array(x_train)\n",
        "    y_train=np.array(y_train)\n",
        "\n",
        "    print(\"Training data size:\",x_train.shape)\n",
        "\n",
        "    adadelta=Adadelta(lr=0.1, rho=0.95, epsilon=1e-06)\n",
        "\n",
        "    # cv_scores=[]\n",
        "    for cv in range(n_cross_val):\n",
        "        print(\"Cross Validation Iteration:\",cv)\n",
        "        model.compile(loss='categorical_crossentropy',optimizer=adadelta,metrics=['accuracy'])\n",
        "        perm=np.random.permutation(x_train.shape[0])\n",
        "        x_train=x_train[perm]\n",
        "        y_train=y_train[perm]\n",
        "        model.fit(x_train,y_train,epochs=n_epochs,batch_size=b_size,validation_split=0.10)\n",
        "        print(\"Training Completed\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "tdiAiVBX1cLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(\n",
        "    model,\n",
        "    speakers,\n",
        "    tst_feat_path,\n",
        "    n_output,\n",
        "    b_size=20\n",
        "):\n",
        "    total_tests=0\n",
        "    t_acc=0\n",
        "    total_frames=0\n",
        "    f_acc=0\n",
        "    confusion_matrix=np.zeros((len(speakers),len(speakers)))\n",
        "\n",
        "    for spk in speakers:\n",
        "        for testcasefile in glob.glob(os.path.join(tst_feat_path, spk + '*')):\n",
        "            total_tests+=1\n",
        "            data=np.genfromtxt(testcasefile, delimiter=',')\n",
        "            labels=np.zeros((data.shape[0],n_output))\n",
        "            labels[:,speakers[spk]]+=1\n",
        "            print(\"Test data size:\",data.shape)\n",
        "            classes = model.predict(data, batch_size=b_size)\n",
        "            pred_labels=[ np.argmax(classes[i]) for i in range(classes.shape[0]) ]\n",
        "\n",
        "            for i in range(len(pred_labels)):\n",
        "                total_frames+=1\n",
        "                confusion_matrix[speakers[spk]][pred_labels[i]]+=1\n",
        "                if pred_labels[i]==speakers[spk]:\n",
        "                    f_acc+=1\n",
        "            if np.argmax(confusion_matrix[speakers[spk]])==speakers[spk]:\n",
        "                t_acc+=1\n",
        "\n",
        "    test_acc=(t_acc/float(total_tests))*100\n",
        "    print(\"Test Accuracy=\",test_acc)\n",
        "    print(confusion_matrix)"
      ],
      "metadata": {
        "id": "aOD3YGdp1fNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=os.path.join(DATA_DIR, \"traindata\")\n",
        "test_data=os.path.join(DATA_DIR, \"testdata\")\n",
        "path_feat_trn=os.path.join(DATA_DIR, \"feat_dnn/train\")\n",
        "all_speakers_tr = feat_ext(path_feat_trn,train_data)\n",
        "print(all_speakers_tr)"
      ],
      "metadata": {
        "id": "-Kc1C4WT1iEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c307b8b8-a156-4e13-f494-e2dd0c29af46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdfb0': 0, 'fjlr0': 1, 'fjlg0': 2, 'feme0': 3, 'fdjh0': 4, 'fgcs0': 5, 'fcmg0': 6, 'fcke0': 7, 'falk0': 8, 'fgrw0': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = dnn_model(\n",
        "    n_input=39,\n",
        "    n_output=len(all_speakers_tr),\n",
        "    n_hidden=70,\n",
        "    speakers=all_speakers_tr,\n",
        "    train_feat_path=path_feat_trn\n",
        ")"
      ],
      "metadata": {
        "id": "sj_KCfaj1jur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd565b18-d834-4b25-bc91-52ebe8b1037b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4160, 39)\n",
            "(4894, 39)\n",
            "(4794, 39)\n",
            "(5334, 39)\n",
            "(4609, 39)\n",
            "(4476, 39)\n",
            "(5237, 39)\n",
            "(4336, 39)\n",
            "(4976, 39)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adadelta.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5681, 39)\n",
            "Training data size: (48497, 39)\n",
            "Cross Validation Iteration: 0\n",
            "Epoch 1/10\n",
            "2183/2183 [==============================] - 6s 2ms/step - loss: 43.1119 - accuracy: 0.1034 - val_loss: 30.3101 - val_accuracy: 0.0955\n",
            "Epoch 2/10\n",
            "2183/2183 [==============================] - 6s 3ms/step - loss: 22.6700 - accuracy: 0.0924 - val_loss: 15.9408 - val_accuracy: 0.1002\n",
            "Epoch 3/10\n",
            "2183/2183 [==============================] - 7s 3ms/step - loss: 12.8808 - accuracy: 0.0956 - val_loss: 9.8516 - val_accuracy: 0.0953\n",
            "Epoch 4/10\n",
            "2183/2183 [==============================] - 10s 5ms/step - loss: 8.7445 - accuracy: 0.0966 - val_loss: 7.9569 - val_accuracy: 0.0979\n",
            "Epoch 5/10\n",
            "2183/2183 [==============================] - 7s 3ms/step - loss: 7.4910 - accuracy: 0.1029 - val_loss: 6.9293 - val_accuracy: 0.1041\n",
            "Epoch 6/10\n",
            "2183/2183 [==============================] - 7s 3ms/step - loss: 6.5148 - accuracy: 0.1080 - val_loss: 6.0572 - val_accuracy: 0.1068\n",
            "Epoch 7/10\n",
            "2183/2183 [==============================] - 8s 3ms/step - loss: 5.7048 - accuracy: 0.1153 - val_loss: 5.3532 - val_accuracy: 0.1157\n",
            "Epoch 8/10\n",
            "2183/2183 [==============================] - 10s 5ms/step - loss: 5.0632 - accuracy: 0.1243 - val_loss: 4.8078 - val_accuracy: 0.1282\n",
            "Epoch 9/10\n",
            "2183/2183 [==============================] - 7s 3ms/step - loss: 4.5728 - accuracy: 0.1348 - val_loss: 4.3938 - val_accuracy: 0.1386\n",
            "Epoch 10/10\n",
            "2183/2183 [==============================] - 10s 5ms/step - loss: 4.1997 - accuracy: 0.1461 - val_loss: 4.0698 - val_accuracy: 0.1501\n",
            "Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_feat_tst=os.path.join(DATA_DIR, \"feat_dnn/test\")\n",
        "all_speakers_tst = feat_ext(path_feat_tst,test_data)\n",
        "test(\n",
        "    model,\n",
        "    all_speakers_tst,\n",
        "    path_feat_tst,\n",
        "    n_output=len(all_speakers_tr),\n",
        "    b_size=20\n",
        ")"
      ],
      "metadata": {
        "id": "JogZJRLG1k1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbd8f4f-23d2-4eb3-89f7-aca592ac5e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fdfb0\n",
            "fjlr0\n",
            "fjlg0\n",
            "feme0\n",
            "fdjh0\n",
            "fgcs0\n",
            "fcmg0\n",
            "fcke0\n",
            "falk0\n",
            "fgrw0\n",
            "Test data size: (988, 39)\n",
            "50/50 [==============================] - 0s 2ms/step\n",
            "Test data size: (1354, 39)\n",
            "68/68 [==============================] - 0s 1ms/step\n",
            "Test data size: (1202, 39)\n",
            "61/61 [==============================] - 0s 1ms/step\n",
            "Test data size: (1282, 39)\n",
            "65/65 [==============================] - 0s 1ms/step\n",
            "Test data size: (1390, 39)\n",
            "70/70 [==============================] - 0s 1ms/step\n",
            "Test data size: (1340, 39)\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "Test data size: (1387, 39)\n",
            "70/70 [==============================] - 0s 2ms/step\n",
            "Test data size: (1255, 39)\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Test data size: (1270, 39)\n",
            "64/64 [==============================] - 0s 1ms/step\n",
            "Test data size: (1391, 39)\n",
            "70/70 [==============================] - 0s 3ms/step\n",
            "Test Accuracy= 30.0\n",
            "[[262.  41.  79. 128.  18. 199.  68.  37.  99.  57.]\n",
            " [150. 132. 158. 110. 117. 111.  53. 170. 142. 211.]\n",
            " [ 57. 109.  95.  51. 202. 130. 124. 140. 117. 177.]\n",
            " [153.  74.  28. 262. 160.  63. 209. 163.  92.  78.]\n",
            " [122.  14.  51.  85. 432.  69. 122. 241. 225.  29.]\n",
            " [149.  67.  57. 142. 232. 176.  86. 157. 136. 138.]\n",
            " [319. 177.  21. 104.  95.  48. 111.  89.  46. 377.]\n",
            " [275. 231.  55.  36.  26.  89. 242.  87.  81. 133.]\n",
            " [ 67.  21. 122.  38. 462.  51.  95. 102. 215.  97.]\n",
            " [ 44. 211. 104.  82. 117.  67. 290. 204.  63. 209.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "\n",
        "plot_model(model, to_file='model.png')\n",
        "Image(\"model.png\")"
      ],
      "metadata": {
        "id": "xHUinf5Z1pHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "62dceeb7-b110-4d89-a6c3-7d359314b3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1a7r40z0A5mD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}